{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Regression & Its Evaluation\n",
        "\n",
        "1. What is Simple Linear Regression?\n",
        "-  Simple Linear Regression is a statistical method that models the relationship between a dependent variable (Y) and a single independent variable (X) by fitting a straight line. The equation is:\n",
        " - Y = b0 + b1*X + ε\n",
        "-  where:\n",
        "\n",
        " - b0 = intercept\n",
        "\n",
        " - b1 = slope\n",
        "\n",
        " - ε = error term\n",
        "\n",
        "Q2. Key assumptions of Simple Linear Regression:\n",
        "- - Linearity – The relationship between X and Y is linear.\n",
        "\n",
        " - Independence – Observations are independent of each other.\n",
        "\n",
        " - Homoscedasticity – Constant variance of residuals.\n",
        "\n",
        " - Normality of errors – Residuals follow a normal distribution.\n",
        "\n",
        " - No multicollinearity – Not applicable for single predictor, but relevant for multiple regression.\n",
        "\n",
        "Q3. What is heteroscedasticity and why is it important?\n",
        "-  Heteroscedasticity occurs when the variance of residuals changes with the value of X.\n",
        "\n",
        " - Impact: It can make coefficient estimates inefficient and affect hypothesis testing.\n",
        "\n",
        " - Importance: Detecting and correcting it ensures accurate prediction intervals and valid significance tests.\n",
        "\n",
        "Q4. What is Multiple Linear Regression?\n",
        "-  Multiple Linear Regression models the relationship between a dependent variable and two or more independent variables.\n",
        "Equation:\n",
        "         Y = b0 + b1*X1 + b2*X2 + ... + bn*Xn + ε\n",
        "\n",
        "Q5. What is polynomial regression and how does it differ from linear regression?\n",
        "-  Polynomial Regression is an extension of linear regression where the relationship between independent and dependent variables is modeled as an nth-degree polynomial.\n",
        "Example:\n",
        "        Y = b0 + b1*X + b2*X^2 + ... + bn*X^n\n",
        " - Linear regression fits a straight line.\n",
        "\n",
        " - Polynomial regression fits a curved line to capture non-linear trends.\n",
        "\n",
        " Q6. n 6: Implement a Python program to fit a Simple Linear Regression model to\n",
        "the following sample data:\n",
        "● X = [1, 2, 3, 4, 5]\n",
        "● Y = [2.1, 4.3, 6.1, 7.9, 10.2]\n",
        "Plot the regression line over the data points."
      ],
      "metadata": {
        "id": "vvcX4o8hXOoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "plt.scatter(X, Y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Slope:\", model.coef_[0])\n"
      ],
      "metadata": {
        "id": "gLey4k2PgL2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.: Fit a Multiple Linear Regression model on this sample data:\n",
        " - ● Area = [1200, 1500, 1800, 2000]\n",
        "● Rooms = [2, 3, 3, 4]\n",
        "● Price = [250000, 300000, 320000, 370000]"
      ],
      "metadata": {
        "id": "mJSOySY_gSYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "Area = [1200, 1500, 1800, 2000]\n",
        "Rooms = [2, 3, 3, 4]\n",
        "Price = [250000, 300000, 320000, 370000]\n",
        "\n",
        "df = pd.DataFrame({'Area': Area, 'Rooms': Rooms, 'Price': Price})\n",
        "\n",
        "X = df[['Area', 'Rooms']]\n",
        "y = df['Price']\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# VIF Calculation\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(\"VIF Results:\\n\", vif_data)\n"
      ],
      "metadata": {
        "id": "Y1ax6ikwgWL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 8. Implement polynomial regression on the following data:\n",
        "● X = [1, 2, 3, 4, 5]\n",
        "3\n",
        "● Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "Fit a 2nd-degree polynomial and plot the resulting curve."
      ],
      "metadata": {
        "id": "gzVqvcoEhO6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, Y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.title(\"Polynomial Regression (Degree 2)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M4yXjyxrhdlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Create a residuals plot for a regression model trained on this data:\n",
        "● X = [10, 20, 30, 40, 50]\n",
        "● Y = [15, 35, 40, 50, 65]\n",
        "Assess heteroscedasticity by examining the spread of residuals."
      ],
      "metadata": {
        "id": "ElNGcH3ihmhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
        "Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "residuals = Y - y_pred\n",
        "\n",
        "plt.scatter(X, residuals, color='purple')\n",
        "plt.axhline(y=0, color='black', linestyle='--')\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals Plot\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qmnl6zrUhtSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Handling heteroscedasticity & multicollinearity\n",
        " 1. Addressing heteroscedasticity:\n",
        "\n",
        "  - Use weighted least squares regression.\n",
        "\n",
        " - Transform dependent variable (log, sqrt).\n",
        "\n",
        " - Identify and remove outliers if necessary.\n",
        "\n",
        " 2. Addressing multicollinearity:\n",
        "\n",
        " - Check VIF values; remove variables with high VIF (>10).\n",
        "\n",
        " - Combine correlated variables.\n",
        "\n",
        " - Use regularization methods like Ridge or Lasso regression.\n",
        "\n",
        " 3. Ensure model robustness:\n",
        "\n",
        " - Cross-validation.\n",
        "\n",
        " - Residual analysis.\n",
        "\n",
        " - Feature scaling."
      ],
      "metadata": {
        "id": "plvNtvIah127"
      }
    }
  ]
}